{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7131822e-45de-48ca-b6ba-e563d4151ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991e7ab-9222-49eb-a933-103cbfee707e",
   "metadata": {},
   "source": [
    "# Step 1: Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9d9d1-a34a-46a2-b7cd-59e652a060dc",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fdcd1771-44c5-471e-a811-315f5348428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('HateSpeech_Kenya.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1307979a-2d9c-47af-981e-b448191a4844",
   "metadata": {},
   "source": [
    "Display basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9eb0007b-9651-4116-aac1-85cc728023ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (48076, 5)\n",
      "Columns: ['hate_speech', 'offensive_language', 'neither', 'Class', 'Tweet']\n",
      "   hate_speech  offensive_language  neither  Class  \\\n",
      "0            0                   0        3      0   \n",
      "1            0                   0        3      0   \n",
      "2            0                   0        3      0   \n",
      "3            0                   0        3      0   \n",
      "4            0                   0        3      0   \n",
      "\n",
      "                                               Tweet  \n",
      "0  ['The political elite are in desperation. Ordi...  \n",
      "1  [\"Am just curious the only people who are call...  \n",
      "2  ['USERNAME_3 the area politicians are the one ...  \n",
      "3  ['War expected in Nakuru if something is not d...  \n",
      "4  ['USERNAME_4 tells kikuyus activists that they...  \n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bcd630-40be-49cf-9e64-6757c45dd053",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "10d29cf1-75b9-4897-9690-d5fd426eac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "hate_speech           0\n",
      "offensive_language    0\n",
      "neither               0\n",
      "Class                 0\n",
      "Tweet                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a081f5-cbfc-4a44-acd3-7d8c56377b47",
   "metadata": {},
   "source": [
    "Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eee48ad1-724e-4e7f-89c7-9e4efe743ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "Class\n",
      "0    36352\n",
      "1     8543\n",
      "2     3181\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_counts = df['Class'].value_counts()\n",
    "print(\"\\nClass distribution:\")\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cfa14-b78f-4be0-a3f3-4c1880d4fb55",
   "metadata": {},
   "source": [
    "Map class values to readable labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3cf63baa-9fcc-451c-8880-a16f2b1adca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    0: \"Neither\",\n",
    "    1: \"Offensive\",\n",
    "    2: \"Hate Speech\"\n",
    "}\n",
    "\n",
    "df['class_label'] = df['Class'].map(class_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a3f0a7-9305-4efa-b5cc-21f5c77e67c4",
   "metadata": {},
   "source": [
    "Visualize class distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19f957a4-9d57-4a91-a506-00616bc3f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='class_label', data=df)\n",
    "plt.title('Distribution of Classes')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Class')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c95365-8f5c-43d7-bd0f-98135913c16e",
   "metadata": {},
   "source": [
    "# Step 2: Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520fd88-7012-461b-8458-b1d270899492",
   "metadata": {},
   "source": [
    "Download NLTK resources if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dccb2bd1-8d52-44fd-946f-547d50ce67b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24879841-a80b-4ed2-8534-c563c0ab6f7e",
   "metadata": {},
   "source": [
    "Initialize lemmatizer and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6422a1bd-5897-4ae9-8836-0d495e659d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove the list formatting if present (specific to this dataset)\n",
    "    text = re.sub(r\"^\\['|'\\]$\", \"\", text)\n",
    "    text = text.replace(\"\\\\\\\"\", \"\")\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove usernames (specific to this dataset)\n",
    "    text = re.sub(r'USERNAME_\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Rejoin tokens\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d821c9c-411c-417a-83a0-3a33d053f564",
   "metadata": {},
   "source": [
    "Apply preprocessing to the Tweet column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386cd1f9-f560-4357-af53-d7de11880e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['Tweet'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da01fde-3568-4092-992b-3756ea0608c5",
   "metadata": {},
   "source": [
    "Compare original and processed text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d2fd9-7412-4b36-aa2c-65b999a4f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOriginal vs Processed text samples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {df['Tweet'].iloc[i]}\")\n",
    "    print(f\"Processed: {df['processed_text'].iloc[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0d4093-ae7d-430d-b8e3-bf57835b962e",
   "metadata": {},
   "source": [
    "# Step 3: Feature Engineering and Data Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7232d-4e74-4478-87d8-2e8df6c69781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 3: Creating features and splitting data...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456fc44c-d2ba-4f02-880e-1e38e6be0b88",
   "metadata": {},
   "source": [
    "Check for and remove empty processed texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3b4cc-de45-49c0-9f9f-8a7e1b6109eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_texts = df['processed_text'].str.strip() == ''\n",
    "print(f\"Number of empty texts after preprocessing: {sum(empty_texts)}\")\n",
    "df = df[~empty_texts].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb4a02-3c34-4d1b-939b-70e39ae30742",
   "metadata": {},
   "source": [
    "Split data into features and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5235f6fd-acc9-40ba-8470-a873732480a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['processed_text']\n",
    "y = df['Class']  # Using the numerical class labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58af39-e6a3-4d2e-b3c8-5f5ded8a1016",
   "metadata": {},
   "source": [
    "Split into training and testing sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50152cd3-e14e-47b3-a1a5-483b8948a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243b3d3-53ab-4fc1-9171-5659157bbe1a",
   "metadata": {},
   "source": [
    "Convert text to numerical features using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20708ab4-e6a2-4b0f-8d38-c0dcb9c5e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=5)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Training features shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Testing features shape: {X_test_tfidf.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167ce4b-6577-44a1-bfbb-2cbe9c7bc252",
   "metadata": {},
   "source": [
    "Save key vocabulary words for later reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb1372-9838-4b4b-adbb-c2ab6e07e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = pd.DataFrame(tfidf_vectorizer.vocabulary_.items(), columns=['Word', 'Index'])\n",
    "top_words = top_words.sort_values('Index').head(20)\n",
    "print(\"\\nTop TF-IDF vocabulary words:\")\n",
    "print(top_words['Word'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906e8aa-a74f-4422-8fd3-97d819213ed4",
   "metadata": {},
   "source": [
    "# Step 4: Model Building and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e96b2-ea24-4552-803c-9d3488fb1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStep 4: Building and evaluating models...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dfcc40-7b04-41c6-8a27-f2f1aa7435de",
   "metadata": {},
   "source": [
    "Define models to test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad86af-e90f-437b-9687-19a4dae60b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, C=1, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'Linear SVM': LinearSVC(C=1, class_weight='balanced', max_iter=10000),\n",
    "    'Multinomial NB': MultinomialNB(alpha=0.1)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a504dd4-389d-46d3-940a-57f0c5d3c332",
   "metadata": {},
   "source": [
    "Train and evaluate each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f90cb-883d-423a-b2a1-e0d10e75ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=list(class_mapping.values()))\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'report': report,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    print(f\"Classification Report:\\n{report}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=list(class_mapping.values()),\n",
    "                yticklabels=list(class_mapping.values()))\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'confusion_matrix_{name.replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74666f38-70df-4973-91f5-2b200d53d9a6",
   "metadata": {},
   "source": [
    "Compare model accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a3d03-3a89-4f86-9bff-0b1867b0bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[name]['accuracy'] for name in model_names]\n",
    "\n",
    "plt.bar(model_names, accuracies)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af516777-3908-4c02-9e2f-12417f07c27c",
   "metadata": {},
   "source": [
    "Identify the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e7a44-dbf7-4933-973c-f9343791420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = max(results, key=lambda k: results[k]['accuracy'])\n",
    "print(f\"\\nBest model: {best_model_name} with accuracy {results[best_model_name]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54798c92-b18b-4452-9878-7ff3e830de23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2746bfb-9e0b-4440-9dfa-59aa9275ecdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a19af3-28f2-4160-9d69-853b2b0509b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44888a-9883-4180-8ed1-d37a7801838e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b2c62-8a8d-46fb-ad3b-a847dfa543d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
