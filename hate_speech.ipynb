{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7131822e-45de-48ca-b6ba-e563d4151ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991e7ab-9222-49eb-a933-103cbfee707e",
   "metadata": {},
   "source": [
    "# Step 1: Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9d9d1-a34a-46a2-b7cd-59e652a060dc",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdcd1771-44c5-471e-a811-315f5348428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('HateSpeech_Kenya.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1307979a-2d9c-47af-981e-b448191a4844",
   "metadata": {},
   "source": [
    "Display basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9eb0007b-9651-4116-aac1-85cc728023ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (48076, 5)\n",
      "Columns: ['hate_speech', 'offensive_language', 'neither', 'Class', 'Tweet']\n",
      "   hate_speech  offensive_language  neither  Class  \\\n",
      "0            0                   0        3      0   \n",
      "1            0                   0        3      0   \n",
      "2            0                   0        3      0   \n",
      "3            0                   0        3      0   \n",
      "4            0                   0        3      0   \n",
      "\n",
      "                                               Tweet  \n",
      "0  ['The political elite are in desperation. Ordi...  \n",
      "1  [\"Am just curious the only people who are call...  \n",
      "2  ['USERNAME_3 the area politicians are the one ...  \n",
      "3  ['War expected in Nakuru if something is not d...  \n",
      "4  ['USERNAME_4 tells kikuyus activists that they...  \n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bcd630-40be-49cf-9e64-6757c45dd053",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10d29cf1-75b9-4897-9690-d5fd426eac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "hate_speech           0\n",
      "offensive_language    0\n",
      "neither               0\n",
      "Class                 0\n",
      "Tweet                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a081f5-cbfc-4a44-acd3-7d8c56377b47",
   "metadata": {},
   "source": [
    "Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eee48ad1-724e-4e7f-89c7-9e4efe743ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "Class\n",
      "0    36352\n",
      "1     8543\n",
      "2     3181\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_counts = df['Class'].value_counts()\n",
    "print(\"\\nClass distribution:\")\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cfa14-b78f-4be0-a3f3-4c1880d4fb55",
   "metadata": {},
   "source": [
    "Map class values to readable labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3cf63baa-9fcc-451c-8880-a16f2b1adca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    0: \"Neither\",\n",
    "    1: \"Offensive\",\n",
    "    2: \"Hate Speech\"\n",
    "}\n",
    "\n",
    "df['class_label'] = df['Class'].map(class_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a3f0a7-9305-4efa-b5cc-21f5c77e67c4",
   "metadata": {},
   "source": [
    "Visualize class distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19f957a4-9d57-4a91-a506-00616bc3f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='class_label', data=df)\n",
    "plt.title('Distribution of Classes')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Class')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c95365-8f5c-43d7-bd0f-98135913c16e",
   "metadata": {},
   "source": [
    "# Step 2: Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520fd88-7012-461b-8458-b1d270899492",
   "metadata": {},
   "source": [
    "Download NLTK resources if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dccb2bd1-8d52-44fd-946f-547d50ce67b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24879841-a80b-4ed2-8534-c563c0ab6f7e",
   "metadata": {},
   "source": [
    "Initialize lemmatizer and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6422a1bd-5897-4ae9-8836-0d495e659d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove the list formatting if present (specific to this dataset)\n",
    "    text = re.sub(r\"^\\['|'\\]$\", \"\", text)\n",
    "    text = text.replace(\"\\\\\\\"\", \"\")\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove usernames (specific to this dataset)\n",
    "    text = re.sub(r'USERNAME_\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Rejoin tokens\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d821c9c-411c-417a-83a0-3a33d053f564",
   "metadata": {},
   "source": [
    "Apply preprocessing to the Tweet column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386cd1f9-f560-4357-af53-d7de11880e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df['processed_text'] = df['Tweet'].apply(preprocess_text)\n",
    "except LookupError as e:\n",
    "    print(f\"NLTK resource error: {e}\")\n",
    "    print(\"Please run the nltk.download() commands above first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da01fde-3568-4092-992b-3756ea0608c5",
   "metadata": {},
   "source": [
    "Compare original and processed text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d2fd9-7412-4b36-aa2c-65b999a4f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOriginal vs Processed text samples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {df['Tweet'].iloc[i]}\")\n",
    "    print(f\"Processed: {df['processed_text'].iloc[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91697d48-4ea3-4b18-bf37-b0acf0a7d156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7232d-4e74-4478-87d8-2e8df6c69781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
